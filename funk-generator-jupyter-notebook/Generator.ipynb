{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests) (2.6)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 7.0MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: urllib3, certifi, chardet, requests\n",
      "Successfully installed certifi-2019.6.16 chardet-3.0.4 requests-2.22.0 urllib3-1.25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('funkemtxt.txt', 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "vocab = sorted(set(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_50'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir ='./training_checkpoints'\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            31232     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 122)            125050    \n",
      "=================================================================\n",
      "Total params: 5,403,258\n",
      "Trainable params: 5,403,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 900\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 0.0595\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "funk_generator = generate_text(model, start_string=u\"som: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "som: As menina gosta\n",
      "\n",
      "Nossa ou não vai tá safadinha\n",
      "De, de sabia o que é pra não se esquecer\n",
      "E na praci no seu corpo nu\n",
      "É se envolver, somos festa em avisando e forma de exemplo\n",
      "Sou todo lado, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em baixo, na casa dos macho\n",
      "Pode ter certeza que lá eu me acho\n",
      "Eu vou dormir lá em\n"
     ]
    }
   ],
   "source": [
    "print(funk_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"http://funk-generator.azurewebsites.net/api/funks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"name\": \"Bomba e café\",\n",
    "        \"author\": \"MC TensorFlow\",\n",
    "        \"letter\":funk_generator\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bomba e café',\n",
       " 'author': 'MC TensorFlow',\n",
       " 'letter': 'baile: Patráculba, matara no chão\\nE quando ela mexe com a bunda no chão\\n\\nPra isso aqui é (Bomba)\\n\\nPara dançar isso aqui é (Bomba)\\nPara mexer isso aqui é (Bomba)\\n\\nPara dançar isso aqui\\nÉ fora que o meu bonde é pra dar um rolé\\n\\nHoje não vai ter café\\nVocê já sabe qual é\\nTática ágil de persuasão\\nMete o louco em quem tá no seu pé\\nE quando cê percebe, linguinha não é sem só de sacanagem\\nVários sapagar minha historia\\nVer o mundo tá dentro do cabelo verde\\nNa gangue do cabelo verde, na gangue do cabelo verde\\nNa gangue do cabelo verde, na gangue do cabelo verde\\nNa gangue do cabelo verde, na gangue do cabelo verde, só o pai [?]\\nNão é só prazer, por isso tu voltou\\nGarota, eu tenho que dizer\\nTodas que sentir prazer sua fogo entender a perfeição é de festi\\n\\nEncarcerados na cama, e sabem gritar!\\n\\nVou marcar um som\\nPra todo lado\\nE nssa mistura na cama até merece um som\\nPlantentama sem dinheiro e procurando gir'}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'content-type': 'application/json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(API_ENDPOINT, data=json.dumps(data), headers=headers) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [201]>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
